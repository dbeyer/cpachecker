# This is an auto-generated file, DO NOT EDIT!
# Run ant to generate it.

# This file is part of CPAchecker,
# a tool for configurable software verification:
# https://cpachecker.sosy-lab.org
#
# SPDX-FileCopyrightText: 2007-2020 Dirk Beyer <https://www.sosy-lab.org>
#
# SPDX-License-Identifier: Apache-2.0

# Possible log levels in descending order 
# (lower levels include higher ones):
# OFF:      no logs published
# SEVERE:   error messages
# WARNING:  warnings
# INFO:     messages
# FINE:     logs on main application level
# FINER:    logs on central CPA algorithm level
# FINEST:   logs published by specific CPAs
# ALL:      debugging information
# Care must be taken with levels of FINER or lower, as output files may
# become quite large and memory usage might become an issue.

# single levels to be excluded from being logged
log.consoleExclude = []

# log level of console output
log.consoleLevel = Level.INFO

# name of the log file
log.file = "CPALog.txt"

# single levels to be excluded from being logged
log.fileExclude = []

# log level of file output
log.level = Level.OFF

# Maximum size of log output strings before they will be truncated. Note that
# truncation is not precise and truncation to small values has no effect. Use
# 0 for disabling truncation completely.
log.truncateSize = 10000

# use colors for log messages on console
log.useColors = true

# disable all default output files
# (any explicitly given file will still be written)
output.disable = false

# directory to put all output files in
output.path = "output/"

# base directory for all paths in default values
rootDirectory = "."

# SPDX-FileCopyrightText: 2020 Dirk Beyer <https://www.sosy-lab.org>
#
# SPDX-License-Identifier: Apache-2.0

# Further options for Boolector in addition to the default options. Format: 
# "Optionname=value" with ’,’ to seperate options. Optionname and value can
# be found in BtorOption or Boolector C Api.Example:
# "BTOR_OPT_MODEL_GEN=2,BTOR_OPT_INCREMENTAL=1".
solver.boolector.furtherOptions = ""

# The SAT solver used by Boolector.
solver.boolector.satSolver = CADICAL
  enum:     [LINGELING, PICOSAT, MINISAT, CMS, CADICAL]

# Counts all operations and interactions towards the SMT solver.
solver.collectStatistics = false

# Default rounding mode for floating point operations.
solver.floatingPointRoundingMode = NEAREST_TIES_TO_EVEN
  enum:     [NEAREST_TIES_TO_EVEN, NEAREST_TIES_AWAY, TOWARD_POSITIVE, TOWARD_NEGATIVE,
             TOWARD_ZERO]

# Export solver queries in SmtLib format into a file.
solver.logAllQueries = false
solver.logfile = no default value

# Further options that will be passed to Mathsat in addition to the default
# options. Format is 'key1=value1,key2=value2'
solver.mathsat5.furtherOptions = ""

# Load less stable optimizing version of mathsat5 solver.
solver.mathsat5.loadOptimathsat5 = false

# Use non-linear arithmetic of the solver if supported and throw exception
# otherwise, approximate non-linear arithmetic with UFs if unsupported, or
# always approximate non-linear arithmetic. This affects only the theories of
# integer and rational arithmetic.
solver.nonLinearArithmetic = USE
  enum:     [USE, APPROXIMATE_FALLBACK, APPROXIMATE_ALWAYS]

# Enable additional assertion checks within Princess. The main usage is
# debugging. This option can cause a performance overhead.
solver.princess.enableAssertions = false

# log all queries as Princess-specific Scala code
solver.princess.logAllQueriesAsScala = false

# file for Princess-specific dump of queries as Scala code
solver.princess.logAllQueriesAsScalaFile = "princess-query-%03d-"

# The number of atoms a term has to have before it gets abbreviated if there
# are more identical terms.
solver.princess.minAtomsForAbbreviation = 100

# Random seed for SMT solver.
solver.randomSeed = 42

# If logging from the same application, avoid conflicting logfile names.
solver.renameLogfileToAvoidConflicts = true

# Double check generated results like interpolants and models whether they
# are correct
solver.smtinterpol.checkResults = false

# Further options that will be set to true for SMTInterpol in addition to the
# default options. Format is 'option1,option2,option3'
solver.smtinterpol.furtherOptions = []

# Which SMT solver to use.
solver.solver = SMTINTERPOL
  enum:     [MATHSAT5, SMTINTERPOL, Z3, PRINCESS, BOOLECTOR, CVC4, YICES2]

# Sequentialize all solver actions to allow concurrent access!
solver.synchronize = false

# Use provers from a seperate context to solve queries. This allows more
# parallelity when solving larger queries.
solver.synchronized.useSeperateProvers = false

# Log solver actions, this may be slow!
solver.useLogger = false

# Activate replayable logging in Z3. The log can be given as an input to the
# solver and replayed.
solver.z3.log = no default value

# Ordering for objectives in the optimization context
solver.z3.objectivePrioritizationMode = "box"
  allowed values: [lex, pareto, box]

# Engine to use for the optimization
solver.z3.optimizationEngine = "basic"
  allowed values: [basic, farkas, symba]

# Require proofs from SMT solver
solver.z3.requireProofs = false

# Whether to use PhantomReferences for discarding Z3 AST
solver.z3.usePhantomReferences = false

# SPDX-FileCopyrightText: 2007-2020 Dirk Beyer <https://www.sosy-lab.org>
# SPDX-FileCopyrightText: 2007-2021 Dirk Beyer <https://www.sosy-lab.org>
# SPDX-FileCopyrightText: 2014-2017 Université Grenoble Alpes
# SPDX-FileCopyrightText: 2020 Dirk Beyer <https://www.sosy-lab.org>
# SPDX-FileCopyrightText: 2021 Dirk Beyer <https://www.sosy-lab.org>
#
# SPDX-License-Identifier: Apache-2.0

# Refiner that SlicingDelegatingRefiner should delegate to
SlicingDelegatingRefiner.refiner = no default value

# maximum number of condition adjustments (-1 for infinite)
adjustableconditions.adjustmentLimit = -1

# number of threads, positive values match exactly, with -1 we use the number
# of available cores or the machine automatically.
algorithm.parallelBam.numberOfThreads = -1

# export number of running RSE instances as CSV
algorithm.parallelBam.runningRSESeriesFile = "RSESeries.csv"

# use a BMC like algorithm that checks for satisfiability after the analysis
# has finished, works only with PredicateCPA
analysis.algorithm.BMC = false

# use CEGAR algorithm for lazy counter-example guided analysis
# You need to specify a refiner with the cegar.refiner option.
# Currently all refiner require the use of the ARGCPA.
analysis.algorithm.CEGAR = false

# use McMillan's interpolation-based model checking algorithm, works only
# with PredicateCPA and large-block encoding
analysis.algorithm.IMC = false

# Use MPI for running analyses in new subprocesses. The resulting reachedset
# is the one of the first analysis returning in time. All other mpi-processes
# will get aborted.
analysis.algorithm.MPI = false

# use MPV algorithm for checking multiple properties
analysis.algorithm.MPV = false

# use a analysis which proves if the program satisfies a specified property
# with the help of an enabler CPA to separate differnt program paths
analysis.algorithm.analysisWithEnabler = false

# use adjustable conditions algorithm
analysis.algorithm.conditionAdjustment = false

# for found property violation, perform fault localization with coverage
analysis.algorithm.faultLocalization.by_coverage = false

# Use fault localization with distance metrics
analysis.algorithm.faultLocalization.by_distance = false

# for found property violation, perform fault localization with trace
# formulas
analysis.algorithm.faultLocalization.by_traceformula = false

# Use McMillan's Impact algorithm for lazy interpolation
analysis.algorithm.impact = false

# use nontermination witness validator to check a violation witness for
# termination
analysis.algorithm.nonterminationWitnessCheck = false

# use PDR algorithm
analysis.algorithm.pdr = false

# use a proof check algorithm to validate a previously generated proof
analysis.algorithm.proofCheck = false

# use a proof check algorithm to validate a previously generated proofand
# extract requirements on a (reconfigurable) HW from the proof
analysis.algorithm.proofCheckAndGetHWRequirements = false

# use a proof check algorithm to validate a previously generated proofand
# read the configuration for checking from the proof
analysis.algorithm.proofCheckReadConfig = false

# use a proof check algorithm that using pcc.strategy=arg.ARG_CMCStrategy to
# validate a previously generated proof
analysis.algorithm.proofCheckWithARGCMCStrategy = false

# do analysis and then check if reached set fulfills property specified by
# ConfigurableProgramAnalysisWithPropertyChecker
analysis.algorithm.propertyCheck = false

# Use termination algorithm to prove (non-)termination. This needs the
# TerminationCPA as root CPA and an automaton CPA with
# termination_as_reach.spc in the tree of CPAs.
analysis.algorithm.termination = false

# collect undefined functions
analysis.algorithm.undefinedFunctionCollector = false

# run the parallel BAM algortihm.
analysis.algorithm.useParallelBAM = false

# If not already done by the analysis, store a found counterexample in the
# ARG for later re-use. Does nothing if no ARGCPA is used
analysis.alwaysStoreCounterexamples = false

# Construct a residual program from condition and verify residual program
analysis.asConditionalVerifier = false

# use a second model checking run (e.g., with CBMC or a different CPAchecker
# configuration) to double-check counter-examples
analysis.checkCounterexamples = false

# use counterexample check and the BDDCPA Restriction option
analysis.checkCounterexamplesWithBDDCPARestriction = false

# do analysis and then check analysis result
analysis.checkProof = false

# use assumption collecting algorithm
analysis.collectAssumptions = false

# Construct the program slice for the given configuration.
analysis.constructProgramSlice = false

# Solely construct the residual program for a given condition/assumption.
analysis.constructResidualProgram = false

# continue analysis after a unsupported code was found on one path
analysis.continueAfterUnsupportedCode = false

# Maximum number of counterexamples to be created.
analysis.counterexampleLimit = 0

# stop CPAchecker after startup (internal option, not intended for users)
analysis.disable = false

# entry function
analysis.entryFunction = "main"

# do analysis and then extract pre- and post conditions for custom
# instruction from analysis result
analysis.extractRequirements.customInstruction = false

# create all potential function pointer call edges
analysis.functionPointerCalls = true

# Create edge for skipping a function pointer call if its value is unknown.
analysis.functionPointerEdgesForUnknownPointer = true

# potential targets for call edges created for function pointer parameter
# calls
analysis.functionPointerParameterTargets = {
          FunctionSet.USED_IN_CODE, FunctionSet.RETURN_VALUE, FunctionSet.EQ_PARAM_TYPES}

# potential targets for call edges created for function pointer calls
analysis.functionPointerTargets = {
          FunctionSet.USED_IN_CODE,
          FunctionSet.RETURN_VALUE,
          FunctionSet.EQ_PARAM_TYPES,
          FunctionSet.EQ_PARAM_SIZES,
          FunctionSet.EQ_PARAM_COUNT}

# What CFA nodes should be the starting point of the analysis?
analysis.initialStatesFor = Sets.newHashSet(InitialStatesFor.ENTRY)

# run interprocedural analysis
analysis.interprocedural = true

# the machine model, which determines the sizes of types like int
analysis.machineModel = LINUX32
  enum:     [LINUX32, LINUX64, ARM, ARM64]

# Use as targets for call edges only those shich are assigned to the
# particular expression (structure field).
analysis.matchAssignedFunctionPointers = false

# If a no target function was assigned to a function pointer, use the origin
# heuristic instead of replacing with empty calls
analysis.matchAssignedFunctionPointers.ignoreUnknownAssignments = false

# memorize previously used (incomplete) reached sets after a restart of the
# analysis
analysis.memorizeReachedAfterRestart = false

# Name of the used analysis, defaults to the name of the used configuration
analysis.name = no default value

# Partition the initial states based on the type of location they were
# created for (see 'initialStatesFor')
analysis.partitionInitialStates = false

# A String, denoting the programs to be analyzed
analysis.programNames = []

# which reached set implementation to use?
# NORMAL: just a simple set
# LOCATIONMAPPED: a different set per location (faster, states with different
# locations cannot be merged)
# PARTITIONED: partitioning depending on CPAs (e.g Location, Callstack etc.)
# PSEUDOPARTITIONED: based on PARTITIONED, uses additional info about the
# states' lattice (maybe faster for some special analyses which use merge_sep
# and stop_sep
analysis.reachedSet = PARTITIONED
  enum:     [NORMAL, LOCATIONMAPPED, PARTITIONED, PSEUDOPARTITIONED, USAGE]

# track more statistics about the reachedset
analysis.reachedSet.withStatistics = false

# Use if you are going to change function with function pionter parameter
analysis.replaceFunctionWithParameterPointer = false

# Functions with function pointer parameter which will be instrumented
analysis.replacedFunctionsWithParameters = {"pthread_create"}

# restart the analysis using a different configuration after unknown result
analysis.restartAfterUnknown = false

# Use heuristics to select the analysis
analysis.selectAnalysisHeuristically = false

# if this option is used, the CFA will be loaded from the given file instead
# of parsed from sourcefile.
analysis.serializedCfaFile = no default value

# Split program in subprograms which can be analyzed separately afterwards
analysis.split.program = false

# stop after the first error has been found
analysis.stopAfterError = true

# create summary call statement edges
analysis.summaryEdges = false

# Enable converting test goals to conditions.
analysis.testGoalConverter = no default value

# Replace thread creation operations with a special function callsso, any
# analysis can go through the function
analysis.threadOperationsTransform = false

# Patterns for detecting block starts (ldv_ like functions)
analysis.traversal.blockFunctionPatterns = {"ldv_%_instance_%"}

# resource limit for the block
analysis.traversal.blockResourceLimit = 1000

# save resources for the block if it is empty
analysis.traversal.blockSaveResources = true

# traverse in the order defined by the values of an automaton variable
analysis.traversal.byAutomatonVariable = no default value

# resource limit for the entry block
analysis.traversal.entryResourceLimit = 100000

# which strategy to adopt for visiting states?
analysis.traversal.order = DFS
  enum:     [DFS, BFS, RAND, RANDOM_PATH, ROUND_ROBIN]

# Exponent of random function.This value influences the probability
# distribution over the waitlist elementswhen choosing the next element.Has
# to be a double in the range [0, INF)
analysis.traversal.random.exponent = 1

# Seed for random values.
analysis.traversal.random.seed = 0

# handle abstract states with more automaton matches first? (only if
# AutomatonCPA enabled)
analysis.traversal.useAutomatonInformation = false

# use blocks and set resource limits for its traversal, blocks are handled in
# DFS order
analysis.traversal.useBlocks = false

# handle states with a deeper callstack first
# This needs the CallstackCPA instance to have any effect.
analysis.traversal.useCallstack = false

# handle more abstract states (with less information) first? (only for
# ExplicitCPA)
analysis.traversal.useExplicitInformation = false

# handle states with more loop iterations first.
analysis.traversal.useLoopIterationCount = false

# handle states with a deeper loopstack first.
analysis.traversal.useLoopstack = false

# handle abstract states with fewer heap objects first? (needs SMGCPA)
analysis.traversal.useNumberOfHeapObjects = false

# handle abstract states with fewer running threads first? (needs
# ThreadingCPA)
analysis.traversal.useNumberOfThreads = false

# Use an implementation of postorder strategy that allows to select a
# secondary strategy that is used if there are two states with the same
# postorder id. The secondary strategy is selected with
# 'analysis.traversal.order'.
analysis.traversal.usePostorder = false

# handle states with fewer loop iterations first.
analysis.traversal.useReverseLoopIterationCount = false

# handle states with a more shallow loopstack first.
analysis.traversal.useReverseLoopstack = false

# Use an implementation of reverse postorder strategy that allows to select a
# secondary strategy that is used if there are two states with the same
# reverse postorder id. The secondary strategy is selected with
# 'analysis.traversal.order'.
analysis.traversal.useReversePostorder = false

# perform a weighted random selection based on the branching depth
analysis.traversal.weightedBranches = false

# perform a weighted random selection based on the depth in the ARG
analysis.traversal.weightedDepth = false

# After an incomplete analysis constructs a residual program which contains
# all program paths which are not fully explored
analysis.unexploredPathsAsProgram = false

# Do not report unknown if analysis terminated, report true (UNSOUND!).
analysis.unknownAsTrue = false

# stop the analysis with the result unknown if the program does not satisfies
# certain restrictions.
analysis.unknownIfUnrestrictedProgram = false

# Use array abstraction by program translation
analysis.useArrayAbstraction = false

# select an analysis from a set of analyses after unknown result
analysis.useCompositionAnalysis = false

# add declarations for global variables before entry function
analysis.useGlobalVars = true

# add loop-structure information to CFA.
analysis.useLoopStructure = true

# Use analyses parallely. The resulting reachedset is the one of the first
# analysis finishing in time. All other analyses are terminated.
analysis.useParallelAnalyses = false

# generate test cases for covered test targets
analysis.useTestCaseGeneratorAlgorithm = false

# converts a witness to an ACSL annotated program
analysis.useWitnessToACSLAlgorithm = false

# Whether to export the CFA with abstracted arrays as DOT file.
arrayAbstraction.cfa.export = true

# DOT file path for CFA with abstracted arrays.
arrayAbstraction.cfa.file = "cfa-abstracted-arrays.dot"

# Use a second delegate analysis run to check counterexamples on the original
# program that contains (non-abstracted) arrays.
arrayAbstraction.checkCounterexamples = false

# Configuration file path of the delegate analysis running on the transformed
# program.
arrayAbstraction.delegateAnalysis = no default value

# Add a threshold to the automaton, after so many branches on a path the
# automaton will be ignored (0 to disable)
assumptions.automatonBranchingThreshold = 0

# write collected assumptions as automaton to file
assumptions.automatonFile = "AssumptionAutomaton.txt"

# If it is enabled, automaton does not add assumption which is considered to
# continue path with corresponding this edge.
assumptions.automatonIgnoreAssumptions = false

# compress the produced assumption automaton using GZIP compression.
assumptions.compressAutomaton = false

# export assumptions as automaton to dot file
assumptions.dotExport = false

# write collected assumptions as automaton to dot file
assumptions.dotFile = "AssumptionAutomaton.dot"

# write collected assumptions to file
assumptions.export = true

# export assumptions collected per location
assumptions.export.location = true

# write collected assumptions to file
assumptions.file = "assumptions.txt"

# If it is enabled, check if a state that should lead to false state indeed
# has successors.
assumptions.removeNonExploredWithoutSuccessors = false

# comma-separated list of files with specifications that should be used 
# in a backwards analysis; used if the analysis starts at the target states!
# (see config/specification/ for examples)
backwardSpecification = []

# Count accesses for the BDD library. Counting works for concurrent accesses.
bdd.countLibraryAccess = false

# Size of the BDD cache in relation to the node table size (set to 0 to use
# fixed BDD cache size).
bdd.javabdd.cacheRatio = 0.1

# Initial size of the BDD cache, use 0 for cacheRatio*initTableSize.
bdd.javabdd.cacheSize = 0

# Initial size of the BDD node table in percentage of available Java heap
# memory (only used if initTableSize is 0).
bdd.javabdd.initTableRatio = 0.001

# Initial size of the BDD node table, use 0 for size based on initTableRatio.
bdd.javabdd.initTableSize = 0

# Measure the time spent in the BDD library. The behaviour in case of
# concurrent accesses is undefined!
bdd.measureLibraryAccess = false

# Which BDD package should be used?
# - java:   JavaBDD (default, no dependencies, many features)
# - sylvan: Sylvan (only 64bit Linux, uses multiple threads)
# - cudd:   CUDD (native library required, reordering not supported)
# - micro:  MicroFactory (maximum number of BDD variables is 1024, slow, but
# less memory-comsumption)
# - buddy:  Buddy (native library required)
# - cal:    CAL (native library required)
# - jdd:    JDD
# - pjbdd:  A java native parallel bdd framework
bdd.package = "JAVA"
  allowed values: [JAVA, SYLVAN, CUDD, MICRO, BUDDY, CAL, JDD, PJBDD]

# Size of the BDD cache in relation to the node table size (set to 0 to use
# fixed BDD cache size).
bdd.pjbdd.cacheRatio = 0.1

# size of the BDD cache.
bdd.pjbdd.cacheSize = 0

# Disable thread safe bdd operations.
bdd.pjbdd.disableThreadSafety = false

# increase factor for resizing tables
bdd.pjbdd.increaseFactor = 1

# Initial size of the BDD node table in percentage of available Java heap
# memory (only used if initTableSize is 0).
bdd.pjbdd.initTableRatio = 0.001

# Initial size of the BDD node table, use 0 for size based on initTableRatio.
bdd.pjbdd.initTableSize = 0

# unique table's concurrency factor
bdd.pjbdd.tableParallelism = 10000

# Number of worker threads, Runtime.getRuntime().availableProcessors()
# default
bdd.pjbdd.threads = Runtime.getRuntime().availableProcessors()

# Use bdd chaining.
bdd.pjbdd.useChainedBDD = false

# Use internal a int based bdd representation.
bdd.pjbdd.useInts = false

# initial variable count
bdd.pjbdd.varCount = 100

# Granularity of the Sylvan BDD operations cache (recommended values 4-8).
bdd.sylvan.cacheGranularity = 4

# Log2 size of the BDD cache.
bdd.sylvan.cacheSize = 24

# Log2 size of the BDD node table.
bdd.sylvan.tableSize = 26

# Number of worker threads, 0 for automatic.
bdd.sylvan.threads = 0

# sequentialize all accesses to the BDD library.
bdd.synchronizeLibraryAccess = false

# Allow reduction of function entries; calculate abstractions always at
# function entries?
blockreducer.allowReduceFunctionEntries = true

# Allow reduction of function exits; calculate abstractions always at
# function exits?
blockreducer.allowReduceFunctionExits = true

# Allow reduction of loop heads; calculate abstractions always at loop heads?
blockreducer.allowReduceLoopHeads = false

# write the reduced cfa to the specified file.
blockreducer.reducedCfaFile = "ReducedCfa.rsf"

# Do at most n summarizations on a node.
blockreducer.reductionThreshold = 100

# If BMC did not find a bug, check whether the bounding did actually remove
# parts of the state space (this is similar to CBMC's unwinding assertions).
bmc.boundingAssertions = true

# If BMC did not find a bug, check which parts of the boundary actually
# reachableand prevent them from being unrolled any further.
bmc.boundingAssertionsSlicing = false

# Check reachability of target states after analysis (classical BMC). The
# alternative is to check the reachability as soon as the target states are
# discovered, which is done if cpa.predicate.targetStateSatCheck=true.
bmc.checkTargetStates = true

# try using induction to verify programs with loops
bmc.induction = false

# Strategy for generating auxiliary invariants
bmc.invariantGenerationStrategy = REACHED_SET
  enum:     [INDUCTION, REACHED_SET, DO_NOTHING]

# k-induction configuration to be used as an invariant generator for
# k-induction (ki-ki(-ai)).
bmc.invariantGeneratorConfig = no default value

# Controls how long the invariant generator is allowed to run before the
# k-induction procedure starts.
bmc.invariantGeneratorHeadStartStrategy = NONE
  enum:     [NONE, AWAIT_TERMINATION, WAIT_UNTIL_EXPENSIVE_ADJUSTMENT]

# Export auxiliary invariants used for induction.
bmc.invariantsExport = no default value

# Propagates the interrupts of the invariant generator.
bmc.propagateInvGenInterrupts = false

# Try to simplify the structure of formulas for the sat check of BMC. The
# improvement depends on the underlying SMT solver.
bmc.simplifyBooleanFormula = false

# Use generalized counterexamples to induction as candidate invariants.
bmc.usePropertyDirection = false

# File name where to put the path program that is generated as input for
# CBMC. A temporary file is used if this is unspecified. If specified, the
# file name should end with '.i' because otherwise CBMC runs the
# pre-processor on the file.
cbmc.dumpCBMCfile = no default value

# maximum time limit for CBMC (use milliseconds or specify a unit; 0 for
# infinite)
cbmc.timelimit = 0ms

# continue analysis after a failed refinement (e.g. due to interpolation)
# other paths may still contain errors that could be found
cegar.continueAfterFailedRefinement = false

# if this score is exceeded by the first analysis, the auxilliary analysis
# will be refined
cegar.domainScoreThreshold = 1024

# Whether to do refinement immediately after finding an error state, or
# globally after the ARG has been unrolled completely.
# whether or not global refinement is performed
cegar.globalRefinement = false

# Max number of refinement iterations, -1 for no limit
cegar.maxIterations = -1

# Which refinement algorithm to use? (give class name, required for CEGAR) If
# the package name starts with 'org.sosy_lab.cpachecker.', this prefix can be
# omitted.
cegar.refiner = no default value

# whether or not to use refinement selection to decide which domain to refine
cegar.useRefinementSelection = false

# Add custom labels to the CFA
cfa.addLabels = false

# Add a unique label for each basic block
cfa.addLabels.atBlocks = true

# Add a unique label before each program exit
cfa.addLabels.atProgramExit = false

# Which functions should be interpreted as encoding assumptions
cfa.assumeFunctions = {"__VERIFIER_assume"}

# dump a simple call graph
cfa.callgraph.export = true

# file name for call graph as .dot file
cfa.callgraph.file = "functionCalls.dot"
cfa.callgraph.fileUsed = "functionCallsUsed.dot"

# how often do we clone a function?
cfa.cfaCloner.numberOfCopies = 5

# while this option is activated, before each use of a PointerExpression, or
# a dereferenced field access the expression is checked if it is 0
cfa.checkNullPointers = false

# Whether to have a single target node per function for all invalid null
# pointer dereferences or to have separate nodes for each dereference
cfa.checkNullPointers.singleTargetPerFunction = true

# When a function pointer array element is written with a variable as index,
# create a series of if-else edges with explicit indizes instead.
cfa.expandFunctionPointerArrayAssignments = false

# export CFA as .dot file
cfa.export = true

# export individual CFAs for function as .dot files
cfa.exportPerFunction = true

# export CFA as C file
cfa.exportToC = false
cfa.exportToC.file = "cfa.c"

# export CFA as .dot file
cfa.file = "cfa.dot"

# By enabling this option the variables that are live are computed for each
# edge of the cfa. Live means that their value is read later on.
cfa.findLiveVariables = false

# how often can a function appear in the callstack as a clone of the original
# function?
cfa.functionCalls.recursionDepth = 5

# Also initialize local variables with default values, or leave them
# uninitialized.
cfa.initializeAllVariables = false

# With this option, all declarations in each function will be movedto the
# beginning of each function. Do only use this option if you arenot able to
# handle initializer lists and designated initializers (like they can be used
# for arrays and structs) in your analysis anyway. this option will otherwise
# create c code which is not the same as the original one
cfa.moveDeclarationsToFunctionStart = false

# Export CFA as pixel graphic to the given file name. The suffix is added
# corresponding to the value of option pixelgraphic.export.formatIf set to
# 'null', no pixel graphic is exported.
cfa.pixelGraphicFile = "cfaPixel"

# export CFA as .ser file (dump Java objects)
cfa.serialize = false
cfa.serializeFile = "cfa.ser.gz"

# Show messages when dead code is encountered during parsing.
cfa.showDeadCode = true

# Remove all edges which don't have any effect on the program
cfa.simplifyCfa = true

# simplify simple const expressions like 1+2
cfa.simplifyConstExpressions = true

# simplify pointer expressions like s->f to (*s).f with this option the cfa
# is simplified until at maximum one pointer is allowed for left- and
# rightHandSide
cfa.simplifyPointerExpressions = false

# A name of thread_create function
cfa.threads.threadCreate = "pthread_create"

# A name of thread_join function
cfa.threads.threadJoin = "pthread_join"

# A name of thread_create_N function
cfa.threads.threadSelfCreate = "pthread_create_N"

# A name of thread_join_N function
cfa.threads.threadSelfJoin = "pthread_join_N"

# This option causes the control flow automaton to be transformed into the
# automaton of an equivalent program with one single loop and an artificial
# program counter.
cfa.transformIntoSingleLoop = false

# Single loop transformation builds a decision tree based on the program
# counter values. This option causes the last program counter value not to be
# explicitly assumed in the decision tree, so that it is only indirectly
# represented by the assumption of falsehood for all other assumptions in the
# decision tree.
cfa.transformIntoSingleLoop.omitExplicitLastProgramCounterAssumption = false

# This option controls the size of the subgraphs referred to by program
# counter values. The larger the subgraphs, the fewer program counter values
# are required. Possible values are  MULTIPLE_PATHS, SINGLE_PATH and
# SINGLE_EDGE, where MULTIPLE_PATHS has the largest subgraphs (and fewest
# program counter values) and SINGLE_EDGE has the smallest subgraphs (and
# most program counter values). The larger the subgraphs, the closer the
# resulting graph will look like the original CFA.
